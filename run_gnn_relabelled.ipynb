{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50f3fe6",
   "metadata": {},
   "source": [
    "## Code for running the graphical neural network\n",
    "\n",
    "gnn_models and supporting files can be found at https://github.com/SJTUBME-QianLab/AutoMetricGNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cbb97d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gnn_models as models\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda as cuda\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from statistics import mean\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from utils import io_utils\n",
    "from sklearn import metrics as metrics\n",
    "\n",
    "import visualize\n",
    "import gnn_models as models\n",
    "from predict import Predict\n",
    "from data import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79dd5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convenient variables\n",
    "RESULT_PATH = Path(\"results/august2022/altered_300\")\n",
    "DATA_PATH = Path(\"data_prep/data\")\n",
    "\n",
    "# tensorboard summary writer\n",
    "WRITER_PATH = Path('runs/august2022/altered_300')\n",
    "tb = SummaryWriter(WRITER_PATH)\n",
    "\n",
    "#set number of iterations\n",
    "n_iter=300\n",
    "#set filenames for input\n",
    "trainstring='relabel3103_train.npy'\n",
    "teststring='relabel3103_test.npy'\n",
    "#trainstring='orig_train.npy'\n",
    "#teststring='orig_test.npy'\n",
    "#set stopping criterion patience\n",
    "patience=10\n",
    "#determine if stopping\n",
    "stopbool=False\n",
    "#length of experiment\n",
    "maxloops=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f84dcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relabel3103_test.npy'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teststring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b099065",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Args(object):\n",
    "    pass\n",
    "\n",
    "args=Args()\n",
    "args.metric_network=\"gnn\"\n",
    "args.dataset=\"AD\"\n",
    "args.train_N_way=3\n",
    "args.test_N_way=3\n",
    "args.test_N_shots =1\n",
    "args.train_N_shots=1\n",
    "args.lr=0.001\n",
    "args.feature_num=221\n",
    "args.clinical_feature_num= 8\n",
    "args.w_feature_num= 213\n",
    "args.w_feature_list=8\n",
    "args.iterations=n_iter\n",
    "args.dec_lr=10000\n",
    "args.log_interval=1\n",
    "args.batch_size=64\n",
    "args.batch_size_train=64\n",
    "args.batch_size_test=64\n",
    "args.test_interval=200\n",
    "#args.random_seed=2021\n",
    "#for original 10\n",
    "args.random_seed=2023\n",
    "#for second 20\n",
    "args.cuda = True\n",
    "args.w_feature_list\n",
    "\n",
    "random_seed = args.random_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6ae4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_seed(seed=random_seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizers, lr, iter, writer=None):\n",
    "    new_lr = lr * (0.5 ** (int(iter / args.dec_lr)))\n",
    "\n",
    "    for optimizer in optimizers:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "\n",
    "    if writer:\n",
    "        writer.add_scalar(\"Learning rate\", new_lr, iter)\n",
    "\n",
    "\n",
    "def train_batch(model, data):\n",
    "    \"\"\"Train a model on selected data sample\"\"\"\n",
    "    [amgnn, softmax_module] = model\n",
    "    [batch_x, label_x, batches_xi, labels_yi, oracles_yi] = data\n",
    "\n",
    "    # slice the first five features which are our risk factors\n",
    "    z_clinical = batch_x[:, 0, 0, 0:args.clinical_feature_num]\n",
    "    zi_s_clinical = [batch_xi[:,0,0,0:args.clinical_feature_num] for batch_xi in batches_xi]\n",
    "\n",
    "    # slice the remaining features after our clinical / risk factors\n",
    "    z_mri_feature = batch_x[:, :, :, args.clinical_feature_num:]\n",
    "    zi_s_mri_feature = [batch_xi[:, :, :, args.clinical_feature_num:] for batch_xi in batches_xi]\n",
    "    adj = amgnn.compute_adj(z_clinical, zi_s_clinical)\n",
    "\n",
    "    inputs = [z_clinical, z_mri_feature, zi_s_clinical, zi_s_mri_feature, labels_yi, oracles_yi, adj]\n",
    "    _, out_logits = amgnn(*inputs)\n",
    "    logsoft_prob = softmax_module.forward(out_logits)\n",
    "\n",
    "    # Loss\n",
    "    label_x_numpy = label_x.cpu().data.numpy()\n",
    "    formatted_label_x = np.argmax(label_x_numpy, axis=1)\n",
    "    formatted_label_x = Variable(torch.LongTensor(formatted_label_x))\n",
    "    if args.cuda:\n",
    "        formatted_label_x = formatted_label_x.cuda()\n",
    "    loss = F.nll_loss(logsoft_prob, formatted_label_x)\n",
    "    loss.backward()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test_one_shot(args, fold, test_root, model, test_samples=50, partition='test', io_path='results/run.log', write_model_graph=False):\n",
    "    io = io_utils.IOStream(io_path)\n",
    "\n",
    "    io.cprint('\\n**** TESTING BEGIN ***' )\n",
    "    root = test_root\n",
    "    data_loader = DataGenerator(root, keys=['CN','MCI','AD'])\n",
    "    [amgnn, *rest] = model\n",
    "    amgnn.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterations = 4\n",
    "    false0=0\n",
    "    false1=0\n",
    "    false2=0\n",
    "    correct0=0\n",
    "    correct1=0\n",
    "    correct2=0\n",
    "    incorrect0=0\n",
    "    incorrect1=0\n",
    "    incorrect2=0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        data = data_loader.get_task_batch(\n",
    "                batch_size=args.batch_size_test,\n",
    "                n_way=args.test_N_way,\n",
    "                num_shots=args.test_N_shots,\n",
    "                cuda=args.cuda\n",
    "        )\n",
    "\n",
    "        Y, y_pred, labels_x_cpu = predict.predict_nodes_using_one_shot(data)\n",
    "       \n",
    "        balancedaccuracy= metrics.balanced_accuracy_score(labels_x_cpu,y_pred)\n",
    "        multi_auc=0\n",
    "        auc = [0 for i in range(3)] \n",
    "    \n",
    "        for i in range(3):    \n",
    "            temptrue=labels_x_cpu==i\n",
    "            temppred=y_pred==i\n",
    "            auc[i] = metrics.roc_auc_score(temptrue,temppred)\n",
    "        \n",
    "        multi_auc=mean(auc)\n",
    "\n",
    "        \n",
    "        for row_i in range(y_pred.shape[0]):\n",
    "            if y_pred[row_i] == labels_x_cpu[row_i]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    labels_x_cpu = Variable(torch.cuda.LongTensor(labels_x_cpu))\n",
    "    loss_test = F.nll_loss(Y, labels_x_cpu)\n",
    "    loss_test_f = float(loss_test)\n",
    "    del loss_test\n",
    "\n",
    "    message = textwrap.dedent(\"\"\"\n",
    "    ***ITERATION FINISHED***\n",
    "    Loss: {}\n",
    "    Correct: {}\n",
    "    Total: {}\n",
    "    Accuracy: {:.3f}%\n",
    "    Balanced Accuracy: {:.3f}%\n",
    "    AUC: {:.3f}\n",
    "    \"\"\".format(loss_test_f, correct, total, (100.0*correct/total), balancedaccuracy,multi_auc))\n",
    "    io.cprint(message)\n",
    "\n",
    "    amgnn.train()\n",
    "    accuracy = 100 * (correct / total)\n",
    "    \n",
    "    return correct, accuracy, loss_test_f,multi_auc,balancedaccuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f1f0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i in range(maxloops):\n",
    "    now = datetime.datetime.now()\n",
    "    now_format = now.strftime('%Y-%m-%d-%H-%M')\n",
    "    now_format=now_format + \"_\" +str(i)\n",
    "    save_path = RESULT_PATH / now_format\n",
    "    if not save_path.exists():\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    io_path = save_path / 'run.log'\n",
    "    io = io_utils.IOStream(io_path)\n",
    "   \n",
    "    tb_path = WRITER_PATH / now_format\n",
    "    if not tb_path.exists():\n",
    "        os.makedirs(tb_path)\n",
    "\n",
    "    tb = SummaryWriter(tb_path)\n",
    "    print(now_format)\n",
    "    print('The result will be saved in :', save_path)\n",
    "    setup_seed(args.random_seed+i)\n",
    "\n",
    "    amgnn = models.create_models(args, cnn_dim1=2)\n",
    "\n",
    "    # initialise softmax and prediction modules\n",
    "    softmax_module = models.SoftmaxModule()\n",
    "    predict = Predict(amgnn, softmax_module, args, io_path)\n",
    "\n",
    "\n",
    "    # NOTE: CNN dimension where one CNN is used for learning the edge weight from the\n",
    "    # absolute difference between each feature of the feature nodes - see notes 1b.\n",
    "    io.cprint(str(amgnn))\n",
    "\n",
    "    if args.cuda:\n",
    "        amgnn.cuda()\n",
    "\n",
    "    weight_decay = 0\n",
    "\n",
    "    opt_amgnn = optim.Adam(amgnn.parameters(), lr=args.lr, weight_decay=weight_decay)\n",
    "    amgnn.train()\n",
    "    counter = 0\n",
    "    total_loss = 0\n",
    "    val_acc, val_acc_aux = 0, 0\n",
    "    test_acc = 0\n",
    "    \n",
    "\n",
    "    root = DATA_PATH / trainstring\n",
    "    data_loader = DataGenerator(root, keys=['CN', 'MCI','AD'])\n",
    "    testacclist=np.zeros(args.iterations)\n",
    "    losslist=np.zeros(args.iterations)\n",
    "    for batch_idx in range(args.iterations):\n",
    "        \n",
    "        data = data_loader.get_task_batch(\n",
    "                batch_size=args.batch_size_train,\n",
    "                n_way=args.train_N_way,\n",
    "                num_shots=args.train_N_shots,\n",
    "                cuda=args.cuda\n",
    "        )\n",
    "        [batch_x, label_x, _, _, batches_xi, labels_yi, oracles_yi] = data\n",
    "\n",
    "        opt_amgnn.zero_grad()\n",
    "\n",
    "        # train model\n",
    "        loss_d_metric = train_batch(model=[amgnn, softmax_module],\n",
    "                                    data=[batch_x, label_x, batches_xi, labels_yi, oracles_yi])\n",
    "        opt_amgnn.step()\n",
    "\n",
    "        adjust_learning_rate(optimizers=[opt_amgnn], lr=args.lr, iter=batch_idx, writer=tb)\n",
    "\n",
    "        # test result output\n",
    "        counter += 1\n",
    "        total_loss += loss_d_metric.item()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            display_str = 'Train Iter: {}'.format(batch_idx)\n",
    "            display_str += '\\tLoss_d_metric: {:.6f}'.format(total_loss / counter)\n",
    "            io.cprint(display_str)\n",
    "            counter = 0\n",
    "            total_loss = 0\n",
    "\n",
    "\n",
    "        # test trained model performance\n",
    "        if (batch_idx + 1) % args.log_interval == 0:\n",
    "\n",
    "            test_samples = 112\n",
    "            test_root = DATA_PATH / teststring\n",
    "            test_correct, test_acc_aux, test_loss_,test_auc,test_balacc = test_one_shot(\n",
    "                            args, 0, test_root, model=[amgnn, softmax_module],\n",
    "                            test_samples=test_samples, partition='test',\n",
    "                            io_path=io_path\n",
    "                        )\n",
    "\n",
    "            # record testing metrics for batch\n",
    "            tb.add_scalar(\"Loss\", test_loss_, batch_idx)\n",
    "            tb.add_scalar(\"Correct\", test_correct, batch_idx)\n",
    "            tb.add_scalar(\"Accuracy\", test_acc_aux, batch_idx)\n",
    "            tb.add_scalar(\"Balanced Accuracy\", test_balacc, batch_idx)\n",
    "            tb.add_scalar(\"AUC\", test_auc, batch_idx)\n",
    "\n",
    "\n",
    "\n",
    "            tb = visualize.record_amgnn_bias_metrics(amgnn, tb)\n",
    "\n",
    "            amgnn.train()\n",
    "            \n",
    "            if test_acc_aux is not None and test_acc_aux >= test_acc:\n",
    "                test_acc = test_acc_aux\n",
    "                # val_acc = val_acc_aux\n",
    "                torch.save(amgnn, save_path / 'amgnn_best_model.pkl')\n",
    "            if args.dataset == 'AD':\n",
    "                io.cprint(\"Best test accuracy {:.4f} \\n\".format(test_acc))\n",
    "            testacclist[batch_idx]=test_acc_aux\n",
    "            losslist[batch_idx]=loss_d_metric\n",
    "            if stopbool and test_acc_aux is not None and batch_idx >10 and all(losslist[i] <= losslist[i+1] for i in range(batch_idx-patience,batch_idx)):\n",
    "                io.cprint(\"Stopping at {0} iterations \\n\".format(batch_idx))\n",
    "                break\n",
    "    tb.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
